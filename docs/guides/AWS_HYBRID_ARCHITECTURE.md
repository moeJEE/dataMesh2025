# ‚òÅÔ∏è Architecture Hybride AWS + Local

**Maximiser 200 USD de cr√©dits AWS pour un projet acad√©mique Data Mesh**

---

## üéØ Strat√©gie

### ‚úÖ Local (Votre Machine - Gratuit)
- JupyterHub (d√©veloppement)
- Trino (queries)
- Grafana (dashboards)
- Outils de d√©veloppement

### ‚òÅÔ∏è AWS (Cloud - 200 USD)
- RDS PostgreSQL (bases de donn√©es production-like)
- S3 (data lake au lieu de Minio)
- IAM (authentication production)
- CloudWatch (monitoring)
- Optionnel: EC2 pour services lourds

---

## üí∞ Estimation Budget AWS (200 USD)

### üìä Configuration Recommand√©e

| Service | Sp√©cifications | Co√ªt/Mois | Dur√©e | Total |
|---------|---------------|-----------|-------|-------|
| **RDS PostgreSQL (Sales)** | db.t3.micro (2vCPU, 1GB RAM) | $15 | 3 mois | $45 |
| **RDS PostgreSQL (Marketing)** | db.t3.micro (2vCPU, 1GB RAM) | $15 | 3 mois | $45 |
| **S3 Storage** | 50GB + 10K requests | $5 | 3 mois | $15 |
| **EC2 (Optionnel)** | t3.medium (2vCPU, 4GB RAM) | $30 | 2 mois | $60 |
| **CloudWatch** | Logs + Metrics | $5 | 3 mois | $15 |
| **Data Transfer** | Sortant vers votre machine | $5 | 3 mois | $15 |
| **TOTAL** | | | | **$195** |

**Marge restante:** 5 USD

---

## üèóÔ∏è Architecture Hybride D√©taill√©e

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    VOTRE MACHINE LOCALE                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ  JupyterHub  ‚îÇ  ‚îÇ    Trino     ‚îÇ  ‚îÇ   Grafana    ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ (Dev & Query)‚îÇ  ‚îÇ (Federated)  ‚îÇ  ‚îÇ (Dashboards) ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                            ‚îÇ                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    üì° Internet (HTTPS)
                             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         AWS CLOUD                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  üóÑÔ∏è  RDS PostgreSQL (Sales)                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Endpoint: sales.xxxxx.rds.amazonaws.com            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Port: 5432                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Storage: 20GB SSD                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  üóÑÔ∏è  RDS PostgreSQL (Marketing)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Endpoint: marketing.xxxxx.rds.amazonaws.com        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Port: 5432                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Storage: 20GB SSD                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  üì¶ S3 Bucket (Data Lake)                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Bucket: datamesh-datalake-[votre-nom]              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Region: us-east-1                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Storage: 50GB                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  üîê IAM (Users & Roles)                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ admin (full access)                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ sales_analyst (Sales DB only)                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ marketing_analyst (Marketing DB only)              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  üìä CloudWatch (Monitoring)                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ DB Performance Insights                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Query Logs                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Cost Tracking                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Plan de D√©ploiement

### √âtape 1: Configuration AWS (1-2 heures)

#### 1.1 Cr√©er les RDS PostgreSQL

```bash
# Sales Database
aws rds create-db-instance \
    --db-instance-identifier datamesh-sales \
    --db-instance-class db.t3.micro \
    --engine postgres \
    --master-username admin \
    --master-user-password YourSecurePassword123! \
    --allocated-storage 20 \
    --publicly-accessible \
    --backup-retention-period 7 \
    --tags Key=Project,Value=DataMeesh

# Marketing Database
aws rds create-db-instance \
    --db-instance-identifier datamesh-marketing \
    --db-instance-class db.t3.micro \
    --engine postgres \
    --master-username admin \
    --master-user-password YourSecurePassword123! \
    --allocated-storage 20 \
    --publicly-accessible \
    --backup-retention-period 7 \
    --tags Key=Project,Value=DataMeesh
```

#### 1.2 Cr√©er S3 Bucket

```bash
# Data Lake
aws s3 mb s3://datamesh-datalake-votrenom --region us-east-1

# Configure public access (d√©mo seulement)
aws s3api put-bucket-versioning \
    --bucket datamesh-datalake-votrenom \
    --versioning-configuration Status=Enabled
```

#### 1.3 Cr√©er IAM Users

```bash
# Admin user
aws iam create-user --user-name datamesh-admin

# Sales analyst
aws iam create-user --user-name datamesh-sales-analyst

# Marketing analyst
aws iam create-user --user-name datamesh-marketing-analyst

# Cr√©er access keys
aws iam create-access-key --user-name datamesh-admin
```

---

### √âtape 2: Configuration Locale (30 minutes)

#### 2.1 Mettre √† jour Trino pour AWS

**Cr√©er:** `config/trino-catalogs-aws.yaml`

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: trino-catalogs-aws
  namespace: data-platform
data:
  # Sales RDS PostgreSQL
  sales.properties: |
    connector.name=postgresql
    connection-url=jdbc:postgresql://sales.xxxxx.us-east-1.rds.amazonaws.com:5432/sales_db
    connection-user=admin
    connection-password=YourSecurePassword123!
    
  # Marketing RDS PostgreSQL
  marketing.properties: |
    connector.name=postgresql
    connection-url=jdbc:postgresql://marketing.xxxxx.us-east-1.rds.amazonaws.com:5432/marketing_db
    connection-user=admin
    connection-password=YourSecurePassword123!
    
  # S3 Data Lake (via Hive)
  hive.properties: |
    connector.name=hive
    hive.metastore=glue  # AWS Glue Catalog (gratuit avec free tier)
    hive.s3.aws-access-key=AKIAIOSFODNN7EXAMPLE
    hive.s3.aws-secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
    hive.s3.region=us-east-1
    hive.non-managed-table-writes-enabled=true
```

#### 2.2 Script de Connexion Python

**Cr√©er:** `examples/connect_aws.py`

```python
#!/usr/bin/env python3
"""
Connect to AWS RDS from local JupyterHub
"""

import psycopg2
from trino.dbapi import connect
import boto3

# ============================================
# 1. Connect to AWS RDS PostgreSQL
# ============================================

def connect_sales_rds():
    """Connect to Sales RDS"""
    conn = psycopg2.connect(
        host='sales.xxxxx.us-east-1.rds.amazonaws.com',
        port=5432,
        database='sales_db',
        user='admin',
        password='YourSecurePassword123!'
    )
    return conn

def connect_marketing_rds():
    """Connect to Marketing RDS"""
    conn = psycopg2.connect(
        host='marketing.xxxxx.us-east-1.rds.amazonaws.com',
        port=5432,
        database='marketing_db',
        user='admin',
        password='YourSecurePassword123!'
    )
    return conn

# ============================================
# 2. Query via Trino (Federated)
# ============================================

def query_via_trino():
    """Federated query across AWS RDS databases"""
    conn = connect(
        host='trino-coordinator.data-platform.svc.cluster.local',
        port=8080,
        user='admin'
    )
    
    cursor = conn.cursor()
    
    # Cross-domain query (Sales + Marketing on AWS)
    cursor.execute("""
        SELECT 
            s.customer_name,
            COUNT(o.order_id) as total_orders,
            SUM(o.total_amount) as revenue,
            m.campaign_name
        FROM sales.public.customers s
        JOIN sales.public.orders o ON s.customer_id = o.customer_id
        LEFT JOIN marketing.public.leads l ON s.email = l.email
        LEFT JOIN marketing.public.campaigns m ON l.campaign_id = m.campaign_id
        GROUP BY s.customer_name, m.campaign_name
        ORDER BY revenue DESC
        LIMIT 10
    """)
    
    results = cursor.fetchall()
    return results

# ============================================
# 3. Upload to S3 Data Lake
# ============================================

def upload_to_s3(file_path, s3_key):
    """Upload file to S3 data lake"""
    s3 = boto3.client(
        's3',
        aws_access_key_id='AKIAIOSFODNN7EXAMPLE',
        aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
        region_name='us-east-1'
    )
    
    bucket = 'datamesh-datalake-votrenom'
    s3.upload_file(file_path, bucket, s3_key)
    
    print(f"‚úÖ Uploaded to s3://{bucket}/{s3_key}")

# ============================================
# 4. Query S3 via Trino
# ============================================

def query_s3_data():
    """Query data lake on S3 via Trino"""
    conn = connect(
        host='trino-coordinator.data-platform.svc.cluster.local',
        port=8080,
        user='admin',
        catalog='hive',
        schema='default'
    )
    
    cursor = conn.cursor()
    
    # Create external table pointing to S3
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS customer_events (
            event_id VARCHAR,
            customer_id BIGINT,
            event_type VARCHAR,
            event_date DATE
        )
        WITH (
            external_location = 's3://datamesh-datalake-votrenom/events/',
            format = 'PARQUET'
        )
    """)
    
    # Query S3 data
    cursor.execute("SELECT * FROM customer_events LIMIT 10")
    return cursor.fetchall()

# ============================================
# Example Usage
# ============================================

if __name__ == "__main__":
    print("üåê Connecting to AWS...")
    
    # Test Sales RDS
    print("\nüìä Querying Sales RDS...")
    sales_conn = connect_sales_rds()
    cursor = sales_conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM customers")
    print(f"Sales customers: {cursor.fetchone()[0]}")
    
    # Test Federated Query
    print("\nüîó Federated query (Sales + Marketing on AWS)...")
    results = query_via_trino()
    for row in results:
        print(row)
    
    # Test S3 upload
    print("\nüì¶ Uploading to S3...")
    upload_to_s3('/home/jovyan/data/dataset.csv', 'datasets/dataset.csv')
    
    print("\n‚úÖ AWS Hybrid Architecture Working!")
```

---

### √âtape 3: Load Sample Data to AWS

**Cr√©er:** `setup/aws/load_data_to_rds.py`

```python
#!/usr/bin/env python3
"""
Load sample data to AWS RDS PostgreSQL
"""

import psycopg2
import os

# Configuration
SALES_RDS_ENDPOINT = os.getenv('SALES_RDS_ENDPOINT', 'sales.xxxxx.rds.amazonaws.com')
MARKETING_RDS_ENDPOINT = os.getenv('MARKETING_RDS_ENDPOINT', 'marketing.xxxxx.rds.amazonaws.com')
DB_PASSWORD = os.getenv('RDS_PASSWORD', 'YourSecurePassword123!')

def load_sales_data():
    """Load sales data to RDS"""
    conn = psycopg2.connect(
        host=SALES_RDS_ENDPOINT,
        port=5432,
        database='sales_db',
        user='admin',
        password=DB_PASSWORD
    )
    
    cursor = conn.cursor()
    
    # Create tables
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS customers (
            customer_id SERIAL PRIMARY KEY,
            customer_name VARCHAR(100),
            email VARCHAR(100),
            phone VARCHAR(20),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # Insert sample data
    cursor.execute("""
        INSERT INTO customers (customer_name, email, phone) VALUES
        ('Alice Johnson', 'alice@example.com', '555-1001'),
        ('Bob Smith', 'bob@example.com', '555-1002'),
        ('Carol White', 'carol@example.com', '555-1003')
    """)
    
    conn.commit()
    print("‚úÖ Sales data loaded to RDS")

def load_marketing_data():
    """Load marketing data to RDS"""
    conn = psycopg2.connect(
        host=MARKETING_RDS_ENDPOINT,
        port=5432,
        database='marketing_db',
        user='admin',
        password=DB_PASSWORD
    )
    
    cursor = conn.cursor()
    
    # Create tables
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS campaigns (
            campaign_id SERIAL PRIMARY KEY,
            campaign_name VARCHAR(100),
            budget DECIMAL(10,2),
            start_date DATE,
            end_date DATE
        )
    """)
    
    # Insert sample data
    cursor.execute("""
        INSERT INTO campaigns (campaign_name, budget, start_date, end_date) VALUES
        ('Spring Sale 2024', 50000.00, '2024-03-01', '2024-03-31'),
        ('Black Friday', 100000.00, '2024-11-25', '2024-11-30')
    """)
    
    conn.commit()
    print("‚úÖ Marketing data loaded to RDS")

if __name__ == "__main__":
    print("üì§ Loading data to AWS RDS...")
    load_sales_data()
    load_marketing_data()
    print("‚úÖ All data loaded!")
```

---

## üí° Avantages de l'Architecture Hybride

### ‚úÖ Pour Votre Projet Acad√©mique:

1. **Professional Look**
   - Architecture cloud-native r√©elle
   - AWS sur le CV
   - D√©mo impressive pour jurys/recruteurs

2. **Cost-Efficient**
   - Local: Gratuit (d√©j√† votre machine)
   - AWS: Seulement 195 USD pour 3 mois
   - 5 USD de marge

3. **Scalability Demo**
   - Montrer comment scaler avec le cloud
   - RDS = production-ready databases
   - S3 = unlimited storage

4. **Real-World Skills**
   - IAM authentication
   - RDS management
   - S3 integration
   - CloudWatch monitoring

5. **RBAC Gratuit**
   - IAM users = RBAC natif AWS
   - Policies pour limiter acc√®s
   - Audit logs automatiques

---

## üìä Monitoring avec CloudWatch

```python
# Dashboard automatique
import boto3

cloudwatch = boto3.client('cloudwatch')

# Cr√©er alarme si co√ªt > 180 USD
cloudwatch.put_metric_alarm(
    AlarmName='DataMeesh-Budget-Alert',
    MetricName='EstimatedCharges',
    Namespace='AWS/Billing',
    Statistic='Maximum',
    Period=86400,
    EvaluationPeriods=1,
    Threshold=180.0,
    ComparisonOperator='GreaterThanThreshold',
    AlarmActions=['arn:aws:sns:us-east-1:xxx:billing-alerts']
)
```

---

## üéì Points Forts pour Pr√©sentation Acad√©mique

### Slide 1: Architecture
```
"Architecture Hybride Moderne:
 ‚Ä¢ Local K8s pour development velocity
 ‚Ä¢ AWS Cloud pour production scalability
 ‚Ä¢ Best of both worlds"
```

### Slide 2: Technologies
```
"Technologies Enterprise:
 ‚Ä¢ AWS RDS (managed databases)
 ‚Ä¢ S3 (data lake)
 ‚Ä¢ IAM (authentication)
 ‚Ä¢ Trino (federated queries)
 ‚Ä¢ JupyterHub (data science)"
```

### Slide 3: Cost Optimization
```
"Budget-Conscious Design:
 ‚Ä¢ Total: 195 USD / 3 mois
 ‚Ä¢ RDS: 90 USD (2 databases)
 ‚Ä¢ S3: 15 USD (50GB)
 ‚Ä¢ Remaining: 90 USD flexible"
```

---

## üöÄ Script de D√©ploiement Complet

**Cr√©er:** `setup/aws/deploy_aws_hybrid.py`

```python
#!/usr/bin/env python3
"""
Deploy DataMeesh AWS Hybrid Architecture
"""

import boto3
import time
import os
from getpass import getpass

def deploy_rds():
    """Deploy RDS PostgreSQL instances"""
    rds = boto3.client('rds', region_name='us-east-1')
    
    password = getpass("Enter RDS master password: ")
    
    print("üöÄ Creating Sales RDS...")
    rds.create_db_instance(
        DBInstanceIdentifier='datamesh-sales',
        DBInstanceClass='db.t3.micro',
        Engine='postgres',
        MasterUsername='admin',
        MasterUserPassword=password,
        AllocatedStorage=20,
        PubliclyAccessible=True,
        BackupRetentionPeriod=7,
        Tags=[{'Key': 'Project', 'Value': 'DataMeesh'}]
    )
    
    print("üöÄ Creating Marketing RDS...")
    rds.create_db_instance(
        DBInstanceIdentifier='datamesh-marketing',
        DBInstanceClass='db.t3.micro',
        Engine='postgres',
        MasterUsername='admin',
        MasterUserPassword=password,
        AllocatedStorage=20,
        PubliclyAccessible=True,
        BackupRetentionPeriod=7,
        Tags=[{'Key': 'Project', 'Value': 'DataMeesh'}]
    )
    
    print("‚è≥ Waiting for RDS instances... (5-10 minutes)")
    
    return password

def deploy_s3():
    """Deploy S3 bucket"""
    s3 = boto3.client('s3', region_name='us-east-1')
    
    bucket_name = f"datamesh-datalake-{os.getenv('USER', 'user')}"
    
    print(f"üöÄ Creating S3 bucket: {bucket_name}")
    s3.create_bucket(Bucket=bucket_name)
    
    # Enable versioning
    s3.put_bucket_versioning(
        Bucket=bucket_name,
        VersioningConfiguration={'Status': 'Enabled'}
    )
    
    return bucket_name

def main():
    print("\n" + "="*70)
    print("‚òÅÔ∏è  DataMeesh - AWS Hybrid Deployment")
    print("="*70 + "\n")
    
    # Deploy
    password = deploy_rds()
    bucket = deploy_s3()
    
    print("\n" + "="*70)
    print("‚úÖ AWS Resources Created!")
    print("="*70 + "\n")
    
    print("üìù Save these credentials:")
    print(f"   RDS Password: {password}")
    print(f"   S3 Bucket: {bucket}")
    print("\n‚è≥ Wait 10 minutes for RDS to be ready, then run:")
    print("   python setup/aws/load_data_to_rds.py")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()
```

---

## üéØ R√©sum√©: Votre Configuration Id√©ale

### Local (Votre Machine):
- JupyterHub
- Trino
- Grafana
- Development

### AWS (200 USD):
- 2x RDS PostgreSQL (Sales + Marketing)
- S3 Data Lake
- IAM (RBAC gratuit!)
- CloudWatch

### Dur√©e:
- **3 mois complets** avec ce budget

### Comp√©tences D√©montr√©es:
- ‚úÖ Data Mesh architecture
- ‚úÖ Hybrid cloud
- ‚úÖ AWS expertise
- ‚úÖ Cost optimization
- ‚úÖ Production-ready setup

---

**Voulez-vous que je cr√©e les scripts de d√©ploiement AWS?** üöÄ

